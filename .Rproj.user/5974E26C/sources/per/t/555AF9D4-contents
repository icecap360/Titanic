#lda and qda need too many parameters for large number of predictors, while 
  #knn and linear regression suffer from curse of dimensionality
#When the outcome is continuous, we call the decision tree method a regression tree.
#Regression and decision trees operate by predicting an outcome variableY by partitioning the predictors
#idea is to build decision tree and at end have different prediction for yhat
#first partition the space into j-non overlapping regions 
  #if xi in Rj then yhat=average of training observations Yi in region xi element of Rj
  #e.g. if we already have partition we have to decide what predictor j touse to make next partition
    #and where to make it within that predictor

#first find predictor j and a value of s that defines 2 new partitions (2 sets) and define 2 new averages
  #pick one that minimizes residual sum squares
#algorithm stops because it sets a complexity parameter (cp) or gain,  
#there is also a minimum number of observations required in a partition before partitioning it further (minsplit in the rpart package). 
  #minsplit arguement, there is also a minimum # of observations in each partitioned (minBucket)
library(rpart)
fit <- rpart(margin ~ ., data = polls_2008)
# visualize the splits 
plot(fit, margin = 0.1)
text(fit, cex = 0.75)
#to plot the predictions use
polls_2008 %>% 
  mutate(y_hat = predict(train_rpart)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col="red")
train_rpart <- train(margin ~ ., method = "rpart", tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)), data = polls_2008)
# prune the tree 
pruned_fit <- prune(fit, cp = 0.01)


#categorical decision trees
#just do a majority vote and use entopy/gini index instead of RSS
  #both of these metric seek partitions of observation into subsets of same class
  #for example if partition has one class,  then gini/entropy will both be zero
#this method is suseptible to changes in training data and not very flexible and easiily overtrain
#random forests address these shortcomings

#RF
#these average multiple decision trees
#2 features, first bootstrap/bagging
  #first build B decision trees using training sets
  #for every observation j in test set we form a prediction yhat using tree Tj
  #to combine trees, fo continuous take average of trees, for categorical do majority vote
  #bootstrap is used to get many decision trees from a single training set
  #you can 
library(randomForest)
fit <- randomForest(margin~., data = polls_2008) 
plot(fit) #plots error vs trees
polls_2008 %>%
  mutate(y_hat = predict(fit, newdata = polls_2008)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_line(aes(day, y_hat), col="red")#, you can also use geom_step(aes(x, y_hat), col=2)
#it is this averaging that permits esstimates to not be step functions i.e. more smoothness



#using cross validation to choose parameter
train_rf_2 <- train(y ~ .,
                    method = "Rborist", #this is a method that is a little bit faster
                    tuneGrid = data.frame(predFixed = 2, minNode = c(3, 50)),
                    data = mnist_27$train)
confusionMatrix(predict(train_rf_2, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]

#to control smoothness we have several ways, one is to limit size of each node
  #we can require the number of points per node to be larger
  # we can use random selection feature to split partitions (we only consider randomly selected subset of predictors at each partition)
    #this reduces correlation between forests, mtry parameter
#A disadvantage of random forests is that we lose interpretability.
#to help with interpretibility, use variable importance 


#Caret Package
#predict.train function
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[["Accuracy"]]
#The train() function automatically uses cross-validation to decide among a few default values of a tuning parameter
#getModelInfo() modelLookup() functions can be used to learn more about a model and the parameters that can be optimized.
ggplot(train_knn, highlight = TRUE) #this highlights parameter that optimizes the algorithm
#by default CV on 25 boot strap samples comprised of 25% of the observations
#knn by default tries k=7,5,9
#tunegrid takes in dataframe with column names as specified by the parameterrs name in model lookup
train_knn$best_Tune #as alternative to final model
control <- trainControl(method = "cv", number = 10, p = .9) #is a way to control cross validation, you can see here we use 10-fold validation
train_knn_cv <- train(y ~ ., method = "knn", 
                      data = mnist_27$train,
                      tuneGrid = data.frame(k = seq(9, 71, 2)),
                      trControl = control)
fit <- train(x,y,method="rpart", tuneGrid = data.frame(
  cp =seq(0,0.1,0.01)), 
  control = rpart.control(minsplit = 0)
)
#we can also get standard deviation of each parameter across validation sets, google
#by reading available models, we see that we can use gamloes method for smoother cond prob
    #this is found in the gam package







# Cool cond prob plot 
plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster(show.legend = FALSE) +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5),color="black")
}

plot_cond_prob(predict(train_knn, mnist_27$true_p, type = "prob")[,2])